# Final Project Proposal: A framework for writing parallel and (doubtfully) distributed graph algorithms
### Jared Roesch and Tristan Konolige

## Introduction
As Haskell programmers we have developed an appreciation for the balance between abstraction and high performance. Often times these two goals are at direct odds, for example the goal of MPI is to not give up performance for the sake of abstraction. We are greedy indviduals and want to have both. One possible approach to this is to build a language that only allows one to express operations that are inherently performant, parallel, distributed, or whatever criteria one is interested in. This is the appraoch taken by MapReduce which only provides the primitives that they can successfully distribute. In their case they have only support `map` which emits a (K, V) pair and then `reduce` which applies a function over all values that map to a specific key. This programming style has allowed multiple projects to build higher level programming abstractions on top, like Scoobi and Scaling. The challenge of making the programming style usable is then put on the library author who is the one that must worry about the restrictions of the core language. 

The current state of the art in large scale parallel computing is filled with approaches that are built upon the abstraction capibilities of C which are poor. We really wanted abstraction and expressivity without giving up performance and so we decided on Haskell is the perfect solution for reaching our goals. It has been made performant in vareity of situations from string operations (recent paper from intel), SDN controllers, and more. We could of alternatively used C++ but Haskell has properties that make our goals much easier to actualize.

An area of active research is the automatic parallelization of languages like Haskell. Haskell is unique in the fact that it is a purely functional and lazy. These properties enable types of reasoning that are not easily performed on other languages. Haskell is filled with many approaches that enable automatically parallel code such as `accelerate` a DSL for GPU programming, `repa for writing code with parallel arrays, as well as approaches to automatica parallelism such as data parallel haskell (for SIMD style programming), the Par monad for speculative paralleism, lightweight system threads for low cpu high througput concurrency/parallelism, and bound OS threads. As well Haskell provides a mature FFI for interfacing with C allowing one to write critical components as needed as well as provide pointers and other low level machinery. 

We want to extend and explore the space of graph algorithms similar to Combinatorial BLAS but exploit some of the type level reasoning and fusion properties Haskell provides. We see our basis as Repa a library for parallel arrays. Repa provides a vareity of tools for dealing with Arrays with varyin representations and allow for GHC to optimize them heavily. This provides us with a battle-tested framework for dealing with parallelism and arrays, but there are still limitations. The biggest of which is that Repa provides no support for nested parallelism and no way to distribute across multiple nodes. We propose a novel extension on top of repa jokingly named doubtful distributed arrays which will provide primitives for dealing with distributed arrays. We initially visualize our distributed arrays as a type of repa repreprsentation which has special properties. We assume all machines to be running identical code similar to the approach taken by OpenMPI and other frameworks. We them implement some combinators over these distributed arrays that allow for one to run node local parallel code on the slice of the array avalible to the machine. Repa provides functionality for "manifesting" arrays which will result in the entire distributed array being pulled into memory at the node. 


